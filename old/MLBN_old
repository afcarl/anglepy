import numpy as np
import theano
import theano.tensor as T
import collections as C
import bnmodels.BNModel as BNModel
import math

class MLBN():
	def __init__(self, n_input, n_hidden, n_output, batchsize):
		
		self.n_input = n_input
		self.n_hidden = n_hidden
		self.n_output = n_output
		self.batchsize = batchsize
		
		# Define parameters
		w1 = T.dmatrix('w1')
		w2 = T.dmatrix('w2')
		b1 = T.dmatrix('b1')
		b2 = T.dmatrix('b2')
		sd2 = T.dmatrix('sd2')
		
		# Latent variables (epsilons)
		eps1 = T.dmatrix('eps1')
		eps2 = T.dmatrix('eps2')
		
		# Data variables
		x = T.dmatrix('x')
		
		# Define internal symbolic variables
		A = np.ones((1, batchsize))
		z = T.tanh(T.dot(w1, eps1) + T.dot(b1, A)) + eps2 * T.dot(sd2, A)
		p = T.nnet.sigmoid(T.dot(w2, z) + T.dot(b2, A)) # p = p(X=1)
		px = x * p + (1-x)*(1-p) # px = p(X=x)
		
		logp_x_eps = T.log(px).sum(axis=0) # logp(x|eps)
		
		logp_eps = (-0.5 * math.log(2 * math.pi) + -0.5 * (eps1**2)).sum(axis=0)
		logp_eps += (-0.5 * math.log(2 * math.pi) + -0.5 * (eps2**2)).sum(axis=0)
		
		joint = logp_x_eps + logp_eps # logp(x, eps)
		
		gw1, gw2, gb1, gb2, gsd2, geps1, geps2 = T.grad(joint.sum(), [w1, w2, b1, b2, sd2, eps1, eps2])
		
		# Compile functions
		self.f_logpx = theano.function([w1, w2, b1, b2, sd2, eps1, eps2, x], joint)
		self.f_g = theano.function([w1, w2, b1, b2, sd2, eps1, eps2, x], [joint, gw1, gw2, gb1, gb2, gsd2, geps1, geps2])
		
		# Named tuples of variables
		#self.W = C.namedtuple('W', ['w1', 'w2', 'b1', 'b2', 'sd1'])
		#self.Z = C.namedtuple('Z', ['eps1', 'eps2'])
		#self.X = C.namedtuple('X', ['x'])
		
	# Initialize parameters
	def init_w(self):
		std = 1e-2
		w = {
			'w1' : np.random.normal(0, std, size=(self.n_hidden, self.n_input)),
			'w2' : np.random.normal(0, std, size=(self.n_output, self.n_hidden)),
			'b1' : np.random.normal(0, std, size=(self.n_hidden, 1)),
			'b2' : np.random.normal(0, std, size=(self.n_output, 1)),
			'sd2' : np.zeros((self.n_hidden, 1))
		}
		return w
	
	# Initialize latent variables
	def init_z(self):
		z = {
			'eps1' : np.random.standard_normal(size=(self.n_input, self.batchsize)),
			'eps2' : np.random.standard_normal(size=(self.n_hidden, self.batchsize)),
		}
		return z
	
	# logp(X=x|z,w)
	def logpx(self, w, z, x):
		return self.f_logpx(w['w1'], w['w2'], w['b1'], w['b2'], w['sd2'], z['eps1'], z['eps2'], x)
	
	# Gradient of log-likelihood w.r.t. parameters and latent variables
	def grad(self, w, z, x):
		gw = {}
		gz = {}
		loglik, gw['w1'], gw['w2'], gw['b1'], gw['b2'], gw['sd2'], gz['eps1'], gz['eps2'] = \
			self.f_g(w['w1'], w['w2'], w['b1'], w['b2'], w['sd2'], z['eps1'], z['eps2'], x)
		
		return loglik, gw, gz
	
	
	