import numpy as np
import theano
import theano.tensor as T
import collections as C
import math
import theano.compile
import bnmodels.ndict as ndict

# converts normal dicts to ordered dicts, ordered by keys
def ordereddict(d):
	return C.OrderedDict(sorted(d.items()))
def ordereddicts(ds):
	result = []
	for d in ds: result.append(ordereddict(d))
	return result
def orderedvals(ds):
	ds = ordereddicts(ds)
	vals = []
	for d in ds:
		vals += d.values()
	return vals

class BNModel():
	def __init__(self, variables, functions, init_w, init_z, n_batch, n_mc):
		
		self.init_w = init_w
		self.init_z = init_z
		self.n_batch = n_batch
		self.n_mc = n_mc
		
		w, z, x = variables()
		
		# Create theano expressions
		logpx, logpz, logpw, confabulation, self.gen_z = functions(w, z, x)
		
		# Monte Carlo estimate
		logpxz = logpx + logpz
		if n_mc > 1:
			C = 1./n_mc * np.ones((n_mc,1))
			logpxz = logpxz.reshape((n_batch,n_mc))
			logpxz_means = logpxz.max(axis=1).dimshuffle(0, 'x')
			pxz = T.exp(logpxz - T.dot(logpxz_means, np.ones((1, n_mc))))
			logpxz = (T.log(T.dot(pxz, C)) + logpxz_means).reshape((1,n_batch))
		
		# Define joint
		joint = logpxz.sum() + logpw
		
		# Get gradient symbols
		w, z, x = ordereddicts((w, z, x))
		allvars = w.values() + z.values() + x.values() # note: '+' concatenates lists
		self.w_keys = w.keys()
		self.z_keys = z.keys()
		gw = T.grad(joint, w.values())
		gz = T.grad(joint, z.values())
		self.f_gw = theano.function(allvars, [joint] + gw)
		self.f_gz = theano.function(allvars, [joint] + gz)
		self.f_gwz = theano.function(allvars, [joint] + gw + gz)
		self.f_logpxz = theano.function(allvars, logpxz)
		self.f_joint = theano.function(allvars, joint)
		confabulation = ordereddict(confabulation)
		self.f_confabulate = theano.function(w.values() + z.values(), confabulation.values())
	
	# NOTE: IT IS ESSENTIAL THAT DICTIONARIES OF SYMBOLIC VARS AND RESPECTIVE NUMPY VALUES HAVE THE SAME KEYS
	# (OTHERWISE FUNCTION ARGUMENTS ARE IN INCORRECT ORDER)
	
	# logp(x,z,w)
	def joint(self, w, z, x):
		if z is None: z = self.gen_z(w)
		return self.f_joint(*orderedvals((w, z, x)))
	
	# logp(x,z|w) (vector)
	def logpxz(self, w, z, x):
		return self.f_logpxz(*orderedvals((w, z, x)))
	
	# Gradient of joint w.r.t. parameters
	def grad_w(self, w, z, x):
		if z is None: z = self.gen_z(w)
		w, z, x = ordereddicts((w, z, x))
		r = self.f_gw(*(w.values() + z.values() + x.values()))
		return r[0], dict(zip(w.keys(), r[1:]))
	
	# Gradient of joint w.r.t. latent variables
	def grad_z(self, w, z, x):
		if z is None: z = self.gen_z(w)
		w, z, x = ordereddicts((w, z, x))
		r = self.f_gz(*(w.values() + z.values() + x.values()))
		return r[0], dict(zip(z.keys(), r[1:]))
	
	# Gradient of joint w.r.t. parameters and latent variables
	def grad_wz(self, w, z, x):
		if z is None: z = self.gen_z(w)
		w, z, x = ordereddicts((w, z, x))
		r = self.f_gwz(*(w.values() + z.values() + x.values()))
		return r[0], dict(zip(w.keys(), r[1:1+len(w)])), dict(zip(z.keys(), r[1+len(w):]))
	
	def confabulate(self, xkeys, w, z=None): # '_x' is just here to know the keys of data
		if z is None: z = self.gen_z(w)
		r = self.f_confabulate(*orderedvals((w, z)))
		return dict(zip(sorted(xkeys), r))
	
	# Helper function that creates tiled version of data 'x' (* n_mc)
	def tiled_x(self, x):
		x_tiled = {}
		for i in x:
			x_tiled[i] = np.tile(x[i].reshape((x[i].size,1)), self.n_mc).reshape((x[i].shape[0], self.n_batch*self.n_mc))
		return x_tiled
	